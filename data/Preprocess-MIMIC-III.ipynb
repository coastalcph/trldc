{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* This file is based on https://github.com/jamesmullenbach/caml-mimic/blob/master/notebooks/dataproc_mimic_III.ipynb\n",
    "\n",
    "* MIMIC-III dataset can be downloaded at https://physionet.org/content/mimiciii/1.4/\n",
    "* The dataset split can be downloaded at https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3\n",
    "~~~\n",
    "wget https://raw.githubusercontent.com/jamesmullenbach/caml-mimic/master/mimicdata/mimic3/train_50_hadm_ids.csv\n",
    "wget https://raw.githubusercontent.com/jamesmullenbach/caml-mimic/master/mimicdata/mimic3/dev_50_hadm_ids.csv\n",
    "wget https://raw.githubusercontent.com/jamesmullenbach/caml-mimic/master/mimicdata/mimic3/test_50_hadm_ids.csv\n",
    "~~~\n",
    "* Extract MIMIC-III files and make sure there are DIAGNOSES_ICD.csv, D_ICD_DIAGNOSES.csv, D_ICD_PROCEDURES.csv, NOTEEVENTS.csv, PROCEDURES_ICD.csv under INPUT_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/data/dai031/Corpora/MIMIC-III/v_1_4\"\n",
    "OUTPUT_DIR = \"/data/dai031/ProcessedData/MIMIC-III/0\"\n",
    "SPLIT_DIR = \"/data/dai031/Corpora/MIMIC-III/split\"\n",
    "!mkdir -p $OUTPUT_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import csv, operator, os, re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine diagnosis and procedure codes and reformat them\n",
    "The codes in MIMIC-III are given in separate files for procedures and diagnoses, and the codes are given without periods, which might lead to collisions if we naively combine them. So we have to add the periods back in the right place."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def reformat_code(code, is_diagnosis):\n",
    "    \"\"\"Generally, procedure codes have dots after the first two digits,\n",
    "    while diagnosis codes have dots after the first three digits.\"\"\"\n",
    "    code = \"\".join(code.split(\".\"))\n",
    "    if is_diagnosis:\n",
    "        if code.startswith(\"E\"):\n",
    "            if len(code) > 4:\n",
    "                code = code[:4] + \".\" + code[4:]\n",
    "        else:\n",
    "            if len(code) > 3:\n",
    "                code = code[:3] + \".\" + code[3:]\n",
    "    else:\n",
    "        code = code[:2] + \".\" + code[2:]\n",
    "    return code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "PROCEDURES_ICD = pd.read_csv(os.path.join(INPUT_DIR, \"PROCEDURES_ICD.csv\"))\n",
    "DIAGNOSES_ICD = pd.read_csv(os.path.join(INPUT_DIR, \"DIAGNOSES_ICD.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DIAGNOSES_ICD[\"absolute_code\"] = DIAGNOSES_ICD.apply(lambda row: str(reformat_code(str(row[4]), True)), axis=1)\n",
    "PROCEDURES_ICD[\"absolute_code\"] = PROCEDURES_ICD.apply(lambda row: str(reformat_code(str(row[4]), False)), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ALL_ICD = pd.concat([DIAGNOSES_ICD, PROCEDURES_ICD])\n",
    "ALL_ICD.to_csv(\"ALL_ICD.csv\", index=False,\n",
    "               columns=[\"ROW_ID\", \"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\", \"absolute_code\"],\n",
    "               header=[\"ROW_ID\", \"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\", \"ICD9_CODE\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "8994"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_ICD = pd.read_csv(\"ALL_ICD.csv\", dtype={\"ICD9_CODE\": str})\n",
    "len(ALL_ICD[\"ICD9_CODE\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize and preprocess raw text\n",
    "Preprocessing time!\n",
    "This will:\n",
    "* Select only discharge summaries and their addenda\n",
    "* change all numbers to 0s\n",
    "* lowercase all tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2083180it [01:04, 32208.17it/s] \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(INPUT_DIR, \"NOTEEVENTS.csv\"), \"r\") as in_f:\n",
    "    with open(\"DISCHARGE_SUMMARIES.csv\", \"w\") as out_f:\n",
    "        out_f.write(\",\".join([\"SUBJECT_ID\", \"HADM_ID\", \"CHARTTIME\", \"TEXT\"]) + \"\\n\")\n",
    "        reader = csv.reader(in_f)\n",
    "        next(reader) # skip the first line\n",
    "\n",
    "        for line in tqdm(reader):\n",
    "            if line[6] == \"Discharge summary\":\n",
    "                text = line[10].strip()\n",
    "                # tokenize, lowercase and normalize numerics\n",
    "                text = re.sub(\"\\d\", \"0\", text.lower())\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                # Mullenbach et al delete numeric-only tokens\n",
    "                text = '\"' + ' '.join(tokens) + '\"'\n",
    "                out_f.write(\",\".join([line[1], line[2], line[4], text]) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "52726"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DISCHARGE_SUMMARIES = pd.read_csv(\"DISCHARGE_SUMMARIES.csv\")\n",
    "len(DISCHARGE_SUMMARIES[\"HADM_ID\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "DISCHARGE_SUMMARIES = DISCHARGE_SUMMARIES.sort_values([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "DISCHARGE_SUMMARIES.to_csv(\"DISCHARGE_SUMMARIES_SORTED.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "! rm DISCHARGE_SUMMARIES.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26992/2553300552.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ALL_ICD = pd.read_csv(\"ALL_ICD.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "(52726, 58976)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_ICD = pd.read_csv(\"ALL_ICD.csv\")\n",
    "ALL_ICD = ALL_ICD.sort_values([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "len(DISCHARGE_SUMMARIES[\"HADM_ID\"].unique()), len(ALL_ICD[\"HADM_ID\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Consolidate labels with discharge summaries\n",
    "Looks like there were some HADM_ID's that didn't have discharge summaries, so they weren't included with our notes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "hadm_ids = set(DISCHARGE_SUMMARIES[\"HADM_ID\"])\n",
    "\n",
    "with open(\"ALL_ICD.csv\", \"r\") as in_f:\n",
    "    with open(\"ALL_ICD_FILTERED.csv\", \"w\") as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        writer.writerow([\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ADMITTIME\", \"DISCHTIME\"])\n",
    "        reader = csv.reader(in_f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            hadm_id = int(row[2])\n",
    "            if hadm_id in hadm_ids:\n",
    "                writer.writerow(row[1:3] + [row[-1], \"\", \"\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26992/3113098409.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ALL_ICD_FILTERED = pd.read_csv(\"ALL_ICD_FILTERED.csv\", index_col=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "52726"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_ICD_FILTERED = pd.read_csv(\"ALL_ICD_FILTERED.csv\", index_col=None)\n",
    "len(ALL_ICD_FILTERED[\"HADM_ID\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "ALL_ICD_FILTERED = ALL_ICD_FILTERED.sort_values([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "ALL_ICD_FILTERED.to_csv(\"ALL_ICD_FILTERED_SORTED.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "! rm ALL_ICD.csv ALL_ICD_FILTERED.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Append labels to notes in a single file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def next_labels(label_filepath):\n",
    "    reader = csv.reader(label_filepath)\n",
    "    next(reader)\n",
    "\n",
    "    first_line = next(reader)\n",
    "\n",
    "    cur_subj = int(first_line[0])\n",
    "    cur_hadm = int(first_line[1])\n",
    "    cur_labels = [first_line[2]]\n",
    "\n",
    "    for row in reader:\n",
    "        subj_id = int(row[0])\n",
    "        hadm_id = int(row[1])\n",
    "        label = row[2]\n",
    "        # keep reading until you hit a new hadm id\n",
    "        if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "            yield cur_subj, cur_hadm, cur_labels\n",
    "            cur_subj = subj_id\n",
    "            cur_hadm = hadm_id\n",
    "            cur_labels = [label]\n",
    "        else:\n",
    "            # add to the labels and move on\n",
    "            cur_labels.append(label)\n",
    "    yield cur_subj, cur_hadm, cur_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def next_notes(note_filepath):\n",
    "    reader = csv.reader(note_filepath)\n",
    "    next(reader)\n",
    "\n",
    "    first_line = next(reader)\n",
    "\n",
    "    cur_subj = int(first_line[0])\n",
    "    cur_hadm = int(first_line[1])\n",
    "    cur_text = first_line[3]\n",
    "\n",
    "    for row in reader:\n",
    "        subj_id = int(row[0])\n",
    "        hadm_id = int(row[1])\n",
    "        text = row[3]\n",
    "        # keep reading until you hit a new hadm id\n",
    "        if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "            yield cur_subj, cur_hadm, cur_text\n",
    "            cur_subj = subj_id\n",
    "            cur_hadm = hadm_id\n",
    "            cur_text = text\n",
    "        else:\n",
    "            # concatenate to the discharge summary and move on\n",
    "            cur_text += \" \" + text\n",
    "    yield cur_subj, cur_hadm, cur_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def concat_data(note_filepath, label_filepath, out_filepath):\n",
    "    with open(label_filepath, \"r\") as label_f:\n",
    "        with open(note_filepath, \"r\") as note_f:\n",
    "            with open(out_filepath, \"w\") as out_f:\n",
    "                writer = csv.writer(out_f)\n",
    "                writer.writerow([\"SUBJECT_ID\", \"HADM_ID\", \"TEXT\", \"LABELS\"])\n",
    "\n",
    "                labels_gen = next_labels(label_f)\n",
    "                notes_gen = next_notes(note_f)\n",
    "\n",
    "                for i, (subj_id, hadm_id, text) in enumerate(notes_gen):\n",
    "                    cur_subj, cur_hadm , cur_labels= next(labels_gen)\n",
    "\n",
    "                    assert cur_hadm == hadm_id\n",
    "                    writer.writerow([subj_id, str(hadm_id), text, \";\".join(cur_labels)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "concat_data(note_filepath=\"DISCHARGE_SUMMARIES_SORTED.csv\",\n",
    "            label_filepath=\"ALL_ICD_FILTERED_SORTED.csv\",\n",
    "            out_filepath=\"DISCHARGE_SUMMARIES_ICD.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "! rm DISCHARGE_SUMMARIES_SORTED.csv ALL_ICD_FILTERED_SORTED.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(138762, 93565687, 52726)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DISCHARGE_SUMMARIES_ICD = pd.read_csv(\"DISCHARGE_SUMMARIES_ICD.csv\")\n",
    "all_tokens = set()\n",
    "num_tokens = 0\n",
    "for row in DISCHARGE_SUMMARIES_ICD.itertuples():\n",
    "    for t in row[3].split():\n",
    "        all_tokens.add(t)\n",
    "        num_tokens += 1\n",
    "\n",
    "len(all_tokens), num_tokens, len(DISCHARGE_SUMMARIES_ICD[\"HADM_ID\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 47723 examples\n",
      "dev set has 1631 examples\n",
      "test set has 3372 examples\n"
     ]
    }
   ],
   "source": [
    "## Create train/dev/test splits\n",
    "split_ids = {}\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    lines = [l.strip() for l in open(os.path.join(SPLIT_DIR, f\"{split}_full_hadm_ids.csv\")).readlines()]\n",
    "    split_ids[split] = set(lines)\n",
    "    print(f\"{split} set has {len(split_ids[split])} examples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore one record (110220), because it has no labels\n",
      "Ignore one record (142890), because it has no labels\n",
      "Ignore one record (109963), because it has no labels\n",
      "Ignore one record (182252), because it has no labels\n"
     ]
    }
   ],
   "source": [
    "split_examples = {k: [] for k in split_ids}\n",
    "\n",
    "with open(\"DISCHARGE_SUMMARIES_ICD.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        hadm_id = row[1]\n",
    "        text = row[2]\n",
    "        labels = [l.strip() for l in row[3].split(\";\") if len(l.strip()) > 0]\n",
    "        labels = list(set(labels))\n",
    "        if len(labels) == 0:\n",
    "            print(f\"Ignore one record ({hadm_id}), because it has no labels\")\n",
    "            continue\n",
    "        example = {\"subject_id\": int(row[0]), \"hadm_id\": hadm_id, \"text\": text ,\"labels\": labels}\n",
    "        if hadm_id in split_ids[\"train\"]:\n",
    "            split_examples[\"train\"].append(example)\n",
    "        elif hadm_id in split_ids[\"dev\"]:\n",
    "            split_examples[\"dev\"].append(example)\n",
    "        else:\n",
    "            assert hadm_id in split_ids[\"test\"]\n",
    "            split_examples[\"test\"].append(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import json, numpy\n",
    "\n",
    "class NumpyJsonEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyJsonEncoder, self).default(obj)\n",
    "\n",
    "def write_list_to_json_file(data, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for i in data:\n",
    "            f.write(f\"{json.dumps(i, cls=NumpyJsonEncoder)}\\n\")\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"full\"), exist_ok=True)\n",
    "\n",
    "for k, v in split_examples.items():\n",
    "    sorted_v = sorted(v, key=lambda i: len(i[\"text\"].split()))\n",
    "    write_list_to_json_file(sorted_v, os.path.join(OUTPUT_DIR, \"full\", f\"{k}.json\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter each split to the top 50 diagnosis/procedure codes\n",
    "\n",
    "first calculate the top k"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "DISCHARGE_SUMMARIES_ICD = pd.read_csv(\"DISCHARGE_SUMMARIES_ICD.csv\")\n",
    "for row in DISCHARGE_SUMMARIES_ICD.itertuples():\n",
    "    for label in str(row[4]).split(\";\"):\n",
    "        counts[label] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"50\"), exist_ok=True)\n",
    "counts_sorted = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "label2idx = {code[0]: i for i, code in enumerate(counts_sorted)}\n",
    "json.dump(label2idx, open(os.path.join(OUTPUT_DIR, \"full\", \"label2idx.json\"), \"w\"))\n",
    "top50labels = [code[0] for code in counts_sorted[:50]]\n",
    "label2idx = {l: i for i, l in enumerate(top50labels)}\n",
    "json.dump(label2idx, open(os.path.join(OUTPUT_DIR, \"50\", \"label2idx.json\"), \"w\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 8066 examples\n",
      "dev set has 1573 examples\n",
      "test set has 1729 examples\n"
     ]
    }
   ],
   "source": [
    "split_ids = {}\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    lines = [l.strip() for l in open(os.path.join(SPLIT_DIR, f\"{split}_50_hadm_ids.csv\")).readlines()]\n",
    "    split_ids[split] = set(lines)\n",
    "    print(f\"{split} set has {len(split_ids[split])} examples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "split_examples = {k: [] for k in split_ids}\n",
    "\n",
    "with open(\"DISCHARGE_SUMMARIES_ICD.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        hadm_id = row[1]\n",
    "        text = row[2]\n",
    "        labels = set(row[3].split(\";\")).intersection(set(top50labels))\n",
    "        example = {\"subject_id\": int(row[0]), \"hadm_id\": hadm_id, \"text\": text ,\"labels\": list(labels)}\n",
    "        if hadm_id in split_ids[\"train\"]:\n",
    "            split_examples[\"train\"].append(example)\n",
    "        elif hadm_id in split_ids[\"dev\"]:\n",
    "            split_examples[\"dev\"].append(example)\n",
    "        elif hadm_id in split_ids[\"test\"]:\n",
    "            split_examples[\"test\"].append(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "for k, v in split_examples.items():\n",
    "    sorted_v = sorted(v, key=lambda i: len(i[\"text\"].split()))\n",
    "    write_list_to_json_file(sorted_v, os.path.join(OUTPUT_DIR, \"50\", f\"{k}.json\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "!rm DISCHARGE_SUMMARIES_ICD.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}